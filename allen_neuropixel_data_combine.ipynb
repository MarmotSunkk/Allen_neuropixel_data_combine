{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4042a17c-6236-4a54-abec-e2185aeb81cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19808\\1149438173.py:8: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from pathlib import Path\n",
    "import json\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "from allensdk.brain_observatory.ecephys.ecephys_session import (\n",
    "    EcephysSession, \n",
    "    removed_unused_stimulus_presentation_columns\n",
    ")\n",
    "from allensdk.brain_observatory.ecephys.visualization import plot_mean_waveforms, plot_spike_counts, raster_plot\n",
    "from allensdk.brain_observatory.visualization import plot_running_speed\n",
    "\n",
    "# tell pandas to show all columns when we display a DataFrame\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a2d0cd-0c82-4845-9252-52080d99243e",
   "metadata": {},
   "source": [
    "## Set the path where data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3149656a-6301-4436-857a-fb758e6bf266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brain_observatory_1.1', 'functional_connectivity']\n"
     ]
    }
   ],
   "source": [
    "# Example cache directory path, it determines where downloaded data will be stored\n",
    "output_dir = r'D:\\Neuroscience'\n",
    "\n",
    "# this path determines where downloaded data will be stored\n",
    "manifest_path = os.path.join(output_dir, \"manifest.json\")\n",
    "\n",
    "cache = EcephysProjectCache.from_warehouse(manifest=manifest_path)\n",
    "\n",
    "print(cache.get_all_session_types())\n",
    "\n",
    "sessions = cache.get_session_table()\n",
    "brain_observatory_type_sessions = sessions[sessions[\"session_type\"] == \"brain_observatory_1.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "757d97f7-06f3-4f00-b59f-849239a95097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brain_observatory_sessions = np.array(brain_observatory_type_sessions.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47fe1778-2caa-4ee7-9709-024c52fa0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为每个刺激长为250ms，所以定义函数把每个刺激分成25个时间单元\n",
    "def calculate_time_unit(row):\n",
    "    time = row['time_since_stimulus_presentation_onset']\n",
    "    if time >= 0.24:\n",
    "        return 25\n",
    "    else:\n",
    "        # 将时间转换为对应的单位，每0.01秒为一个单位，即使时间采样率为100Hz，从1开始编号\n",
    "        return np.floor(time / 0.01) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a60bc-aac4-4450-a5eb-7a7e6715fe5c",
   "metadata": {},
   "source": [
    "对每一个stimulus_presentation_id，基于time_since_stimulus列，如果time_since_stimulus <= 0.01，则time unit为1，如果0.01 < time_since_stimulus <=0.02，则time unit为2，....，如果time_since_stimulus >= 0.24，则time unit为25。这样做有一个前提，每个stimulus都必须有0.24s之后的刺激，因为如果没有的话，此次stimulus就只有24个time unit而不是25个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad32a2b-8a10-4f9b-a024-bbd2e2e015a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_times(session_id):\n",
    "    # 获取session表\n",
    "    session = cache.get_session_data(session_id)\n",
    "    ns_presentation_ids = session.stimulus_presentations.loc[\n",
    "        (session.stimulus_presentations['stimulus_name'] == 'natural_scenes')\n",
    "    ].index.values\n",
    "    \n",
    "    # 获取初始times表(列不全，需要添加其他信息)\n",
    "    times = session.presentationwise_spike_times(\n",
    "        stimulus_presentation_ids=ns_presentation_ids,\n",
    "    )\n",
    "    \n",
    "    # 获取units表\n",
    "    units = session.units\n",
    "    \n",
    "    # 该类型刺激开始的时间点：times.index[0] - times['time_since_stimulus_presentation_onset'].loc[times.index[0]]\n",
    "    # 计算每一行所属的block(每个session的natural scense分为3个block)\n",
    "    times['spike_time'] = times.index\n",
    "    times['spike_time'] = times['spike_time'].astype('float64')\n",
    "    times['time_diff'] = times['spike_time'].diff()\n",
    "    times['is_new_block'] = (times['time_diff'] > 1).astype(int)\n",
    "    times['natural_scenes_block_id'] = times['is_new_block'].cumsum()\n",
    "    times.drop(['time_diff', 'is_new_block'], axis = 1, inplace = True)\n",
    "\n",
    "    flag = 0\n",
    "    for i in times.groupby('stimulus_presentation_id')['time_since_stimulus_presentation_onset'].max():\n",
    "        if i < 0.24:\n",
    "            flag = 1\n",
    "            print('对于session {}, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。'.format(session_id))\n",
    "    if flag == 0:\n",
    "        print('对于session {}, 每个stimulus在每个time_unit内均有神经反应。'.format(session_id))\n",
    "    \n",
    "    # 应用函数创建新列\n",
    "    times['time_unit'] = times.apply(calculate_time_unit, axis=1).astype(int)\n",
    "\n",
    "    # 时间补齐\n",
    "    for index, item in times.groupby('stimulus_presentation_id')['time_unit'].max().iteritems():\n",
    "        if item < 25:\n",
    "            for time_unit in range(item + 1, 26):\n",
    "                new_row = {'stimulus_presentation_id': index, 'time_unit': time_unit}\n",
    "                times = times.append(new_row, ignore_index = True)\n",
    "    \n",
    "    # 计算每个stimulus_presentation_id的time_unit最大值累加偏移量\n",
    "    offsets = times.groupby('stimulus_presentation_id')['time_unit'].max().cumsum().shift(1).fillna(0)\n",
    "    \n",
    "    # 将偏移量应用到每个time_unit上\n",
    "    times['adjusted_time_unit'] = times['time_unit'] + times['stimulus_presentation_id'].map(offsets)\n",
    "    times['adjusted_time_unit'] = times['adjusted_time_unit'].astype(int)\n",
    "    \n",
    "    # 从session.unit表中引入cluster_id，使每一列代表一个神经元。要用left_join，因为没有unit激活的时间单元自然不会有cluster_id，但希望保留这一行\n",
    "    units = session.units\n",
    "    times = pd.merge(times, units[['cluster_id', 'structure_acronym']], \n",
    "                    left_on = 'unit_id', right_on = units.index, how = 'left').sort_values(by = 'time_unit')\n",
    "\n",
    "    # 按列排序\n",
    "    times = times.sort_values(by = ['stimulus_presentation_id', 'adjusted_time_unit'], ascending = [True, True])\n",
    "\n",
    "    return times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "039a8737-08ac-4099-af17-ca70ab88e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_df_annotation(sessions, times, session_id, start_neuron_num):\n",
    "    \n",
    "    # 数据透视表得到每一个time_unit，每一个神经元的激活次数\n",
    "    df = pd.pivot_table(times, index = ['stimulus_presentation_id', 'adjusted_time_unit'], columns = ['cluster_id'], aggfunc = 'size')\n",
    "    df = df.fillna(0).astype(int)\n",
    "\n",
    "    # 取出原来的第一级索引和第二级索引\n",
    "    level_1_index = df.index.get_level_values(0)\n",
    "    level_2_index = df.index.get_level_values(1)\n",
    "    \n",
    "    # 创建完整的第二级索引范围\n",
    "    full_level_2_range = np.arange(1, 148751)\n",
    "    \n",
    "    # 找出缺失的第二级索引, 如果有缺失值，进入补齐过程\n",
    "    missing_level_2 = np.setdiff1d(full_level_2_range, level_2_index)\n",
    "    if len(missing_level_2) != 0:\n",
    "        \n",
    "        # 初始化一个空的DataFrame，用于存放缺失的行\n",
    "        missing_rows = []\n",
    "        for missing in missing_level_2:\n",
    "            # 找到缺失索引之前的第一级索引\n",
    "            prev_index = np.where(level_2_index < missing)[0]\n",
    "        \n",
    "            if len(prev_index) > 0:\n",
    "                prev_level_1_index = level_1_index[prev_index[-1]]\n",
    "            else:\n",
    "                prev_level_1_index = np.nan  # 若没有前一个索引值，则设置为 NaN\n",
    "            # 创建一个新行，使用prev_level_1_index作为第一级索引，missing作为第二级索引，值全为0\n",
    "            missing_rows.append((prev_level_1_index, missing))\n",
    "        # 创建一个新的DataFrame包含这些缺失的行\n",
    "        missing_df = pd.DataFrame(0, index=pd.MultiIndex.from_tuples(missing_rows, names=('level_1', 'level_2')), columns=df.columns)\n",
    "        # 将原来的DataFrame和缺失的DataFrame合并\n",
    "        df = pd.concat([df, missing_df]).sort_index()\n",
    "    \n",
    "\n",
    "    # 获取session表\n",
    "    session = cache.get_session_data(session_id)\n",
    "    ns_presentation_ids = session.stimulus_presentations.loc[\n",
    "        (session.stimulus_presentations['stimulus_name'] == 'natural_scenes')\n",
    "    ].index.values\n",
    "    \n",
    "    # 获取units表\n",
    "    units = session.units\n",
    "    \n",
    "    # 建立annotation file记录元数据\n",
    "    annotation = units[['cluster_id', 'structure_acronym']].drop_duplicates()\n",
    "    annotation['session'] = session_id\n",
    "    annotation = annotation.sort_values(by = 'cluster_id')\n",
    "    \n",
    "    # 设置一个字典，给每一个神经元起名字\n",
    "    sorted_cluster_id = np.sort(annotation['cluster_id'].unique())\n",
    "    dict_cluster_id = {}\n",
    "    for i in range(len(sorted_cluster_id)):\n",
    "        dict_cluster_id[sorted_cluster_id[i]] = 'neuron' + '_' + str(i + start_neuron_num)\n",
    "    \n",
    "    # 给annotation file的神经元起别名\n",
    "    annotation['cluster_id'] = annotation['cluster_id'].replace(dict_cluster_id)\n",
    "    \n",
    "    # 给df的神经元起别名\n",
    "    df.rename(columns = dict_cluster_id, inplace = True)\n",
    "\n",
    "    annotation['specimen_id'] = sessions['specimen_id'].loc[session_id]\n",
    "    annotation['session_type'] = sessions['session_type'].loc[session_id]\n",
    "    annotation['age_in_days'] = sessions['age_in_days'].loc[session_id]\n",
    "    annotation['sex'] = sessions['sex'].loc[session_id]\n",
    "    annotation['full_genotype'] = sessions['full_genotype'].loc[session_id]\n",
    "\n",
    "    # 记录最大的neuron number，防止不同session的neuron重叠\n",
    "    max_neuron_num = len(sorted_cluster_id) + start_neuron_num - 1\n",
    "    \n",
    "    return df, annotation, max_neuron_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52d34162-f09b-487a-b025-36c6c756ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉session 737581020因为只有5912个natural scenes stimulus\n",
    "brain_observatory_sessions = np.array(brain_observatory_sessions.tolist()[:4] + brain_observatory_sessions.tolist()[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47bd903b-6775-432f-b310-9a8fa5756d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([715093703, 719161530, 721123822, 732592105, 739448407, 742951821,\n",
       "       743475441, 744228101, 746083955, 750332458, 750749662, 751348571,\n",
       "       754312389, 754829445, 755434585, 756029989, 757216464, 757970808,\n",
       "       758798717, 759883607, 760345702, 760693773, 761418226, 762120172,\n",
       "       762602078, 763673393, 773418906, 791319847, 797828357, 798911424,\n",
       "       799864342])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_observatory_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "679dd0d5-fac6-4bd7-81e5-7c19b2eeaf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对于session 715093703, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 719161530, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 721123822, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 732592105, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 739448407, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 739448407, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 739448407, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 739448407, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 739448407, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 739448407, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 742951821, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 743475441, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 744228101, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 746083955, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 750332458, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 750749662, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 751348571, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 754312389, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 754312389, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 754829445, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 755434585, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 756029989, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 757216464, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 757970808, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 758798717, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 759883607, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 760345702, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 760693773, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 761418226, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 762120172, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 762602078, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 763673393, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 773418906, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 791319847, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 797828357, 存在stimulus在刺激末尾没有unit产生神经反应, 在对应stimulus末尾的time_unit加上全0行以补齐。\n",
      "对于session 798911424, 每个stimulus在每个time_unit内均有神经反应。\n",
      "对于session 799864342, 每个stimulus在每个time_unit内均有神经反应。\n"
     ]
    }
   ],
   "source": [
    "# 起始编号为0，之后每次编号为前一个session最大编号+1\n",
    "start_neuron_num = 0\n",
    "for session_id in brain_observatory_sessions:\n",
    "    times = calculate_times(session_id)\n",
    "    df, annotation, max_neuron_num = calculate_df_annotation(sessions, times, session_id, start_neuron_num)\n",
    "    df.to_csv(r'D:\\Neuroscience\\neuropixel_data\\stimulus_response(individual_sessions)\\session_{}.csv'.format(session_id))\n",
    "    annotation.to_csv(r'D:\\Neuroscience\\neuropixel_data\\annotation\\annotation_{}.csv'.format(session_id))\n",
    "    start_neuron_num = max_neuron_num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8941dade-d4f7-4ced-a0fb-585279611892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义包含 CSV 文件的文件夹路径\n",
    "folder_path = r'D:\\Neuroscience\\neuropixel_data\\stimulus_response(individual_sessions)'\n",
    "\n",
    "# 获取文件夹下所有 CSV 文件的文件名\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "dataframes = []\n",
    "# 读取每个 CSV 文件并添加到列表中\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path, dtype='int32')  # 使用较低内存的 dtype\n",
    "    dataframes.append(df)\n",
    "\n",
    "# 横向合并所有 DataFrame\n",
    "combined_df = pd.concat(dataframes, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73389f3-02c5-4124-af42-855dda7d8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(r'D:\\Neuroscience\\neuropixel_data\\stimulus_response(individual_sessions)\\all_sessions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8abcd349-d524-430a-b9f7-f1bc5e9d63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义包含 CSV 文件的文件夹路径\n",
    "folder_path = r'D:\\Neuroscience\\neuropixel_data\\annotation'\n",
    "\n",
    "# 获取文件夹下所有 CSV 文件的文件名\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "dataframes = []\n",
    "# 读取每个 CSV 文件并添加到列表中\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path) \n",
    "    dataframes.append(df)\n",
    "\n",
    "# 纵向合并所有 DataFrame\n",
    "combined_annotation = pd.concat(dataframes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334265ff-630e-403e-a8b7-903599055280",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_annotation.to_csv(r'D:\\Neuroscience\\neuropixel_data\\annotation\\all_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f490cb3-f57d-42d1-b156-1ba273ed740a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5912,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950,\n",
       " 5950]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 确定每个session包含多少natural scenes stimulus\n",
    "# session_stimulus_num = []\n",
    "# for session_id in brain_observatory_sessions:\n",
    "#     session = cache.get_session_data(session_id)\n",
    "#     ns_presentation_ids = session.stimulus_presentations.loc[\n",
    "#         (session.stimulus_presentations['stimulus_name'] == 'natural_scenes')\n",
    "#     ].index.values\n",
    "#     times = session.presentationwise_spike_times(\n",
    "#     stimulus_presentation_ids=ns_presentation_ids,\n",
    "#     )\n",
    "#     stimulus_num = len(times.groupby('stimulus_presentation_id').count())\n",
    "#     session_stimulus_num.append(stimulus_num)\n",
    "# session_stimulus_num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
